from __future__ import print_function
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten,BatchNormalization
from keras.layers import Conv2D, MaxPooling2D
import numpy as np
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale
from keras.preprocessing.image import ImageDataGenerator # data augmentation

def renorm (y,value,a=0,b=2062):
    for j in range(a,b):
        for i in range(0, 10):
            if (i == value):
                y[j][i] = 1
            else:
                y[j][i] = 0  # 9
    new_y=y
    return new_y

batch_size = 4
num_classes = 10
epochs = 100

# input image dimensions
img_size = 64
# Lets load in the data
X = np.load('X_custom.npy')
y = np.load('Y_custom.npy')
#y=renorm(y,9,0,204)
#y=renorm(y,0,204,409)
#y=renorm(y,7,409,615)
#y=renorm(y,6,615,822)
#y=renorm(y,1,822,1028)
#y=renorm(y,8,1028,1236)
#y=renorm(y,4,1236,1443)
#y=renorm(y,3,1443,1649)
#y=renorm(y,2,1649,1845)
#y=renorm(y,5,1845,2062)
print('X shape : {}  Y shape: {}'.format(X.shape, y.shape))
#X=whitening(X)
#for i in range(0,2062):
    #X[i] = X[i].astype(float)
    #X[i]=preprocessing.scale(X[i])
# create a data generator using Keras image preprocessing
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=8)
#Xtrain,Xval,ytrain,yval=train_test_split(Xtrain,ytrain,test_size=0.1,random_state=8)

# add another axis representing grey-scale
Xtest = Xtest[:,:,:,np.newaxis]
Xtrain=Xtrain[:,:,:,np.newaxis]
#Xval=Xval[:,:,:,np.newaxis]
print('Xtest shape : {}  Ytest shape: {}'.format(Xtest.shape, ytest.shape))
# build our CNN
model = Sequential()

# Convolutional Blocks: (1) Convolution, (2) Activation, (3) Pooling
model.add(Conv2D(input_shape=(img_size, img_size, 1), filters=64, kernel_size=(4,4), strides=(2),activation='relu'))
#outputs a (20, 20, 32) matrix
#model.add(BatchNormalization(axis=1))
model.add(Dropout(0.5))
model.add(Conv2D(filters=64, kernel_size=(4,4), strides=(1),activation='relu'))
#outputs a (8, 8, 32) matrix
#model.add(BatchNormalization(axis=1))
model.add(Dropout(0.5))
model.add(MaxPooling2D(pool_size=4))
# dropout helps with over fitting by randomly dropping nodes each epoch
model.add(Flatten())
#model.add(BatchNormalization(axis=1))
model.add(Dense(256, activation='relu'))

model.add(Dropout(0.6))
model.add(Dense(num_classes, activation='softmax'))
#Adadelta
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(),
              metrics=['accuracy'])
early_stopping=EarlyStopping(monitor='val_acc', patience=5)
checkpointer = ModelCheckpoint(filepath='model_check.h5', verbose=1,save_best_only=True)

#datagen = ImageDataGenerator(
    #rotation_range=20,
    #width_shift_range=0.2,
    #height_shift_range=0.2,
#horizontal_flip=True,
 #   vertical_flip=True)
# fit the model on the batches generated by datagen.flow()---most parameters similar to model.fit
#datagen.fit(Xtrain)
#history=model.fit_generator(datagen.flow(Xtrain, ytrain, batch_size=4),
 #                   steps_per_epoch=len(Xtrain) / 4, epochs=epochs,verbose=1,validation_data=(Xval,yval),
  #                          callbacks=[early_stopping,checkpointer])


history=model.fit(Xtrain, ytrain,
          batch_size=batch_size,
         epochs=epochs,
        verbose=1,
         validation_split=0.1,
         callbacks=[early_stopping,checkpointer])

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.savefig('picture.png')
plt.show()
score = model.evaluate(Xtest, ytest, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
model.save('sign_lang_model.h5')
